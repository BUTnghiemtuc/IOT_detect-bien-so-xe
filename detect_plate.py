"""
detect_plate.py
- D√πng YOLOv8 ƒë·ªÉ detect bi·ªÉn s·ªë
- D√πng PaddleOCR ƒë·ªÉ ƒë·ªçc ch·ªØ tr√™n bi·ªÉn
- X·ª≠ l√Ω 1 file ·∫£nh ho·∫∑c c·∫£ folder
- L∆∞u crop v√†o ./outputs/crops/ v√† k·∫øt qu·∫£ JSON ./outputs/results.json
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple

import cv2
import numpy as np
from ultralytics import YOLO
from tqdm import tqdm

# PaddleOCR (fast, nhi·ªÅu ng√¥n ng·ªØ). N·∫øu kh√¥ng mu·ªën c√†i PaddleOCR, c√≥ th·ªÉ chuy·ªÉn sang pytesseract.
try:
    from paddleocr import PaddleOCR
    PADDLE_AVAILABLE = True
except Exception:
    PADDLE_AVAILABLE = False

# ---- Default c·∫•u h√¨nh cho b·∫°n ----
MODEL_DEFAULT = r"D:\IOT\license_plate_detector.pt"   # model bi·ªÉn s·ªë c·ªßa b·∫°n
SOURCE_DEFAULT = r"D:\IOT\captures_serial"            # th∆∞ m·ª•c ·∫£nh (·∫£nh t·ª´ ESP32-CAM)
OUT_DIR_DEFAULT = "outputs"
CONF_DEFAULT = 0.25
OCR_LANG_DEFAULT = "en"   # bi·ªÉn s·ªë ch·ªß y·∫øu l√† ch·ªØ s·ªë/latin, en l√† ·ªïn
USE_PADDLE_DEFAULT = True

# ---- Model cache helper ----
_MODEL_CACHE: Dict[str, YOLO] = {}


def get_yolo_model(model_path: str):
    """
    Load YOLO model once and cache it.
    model_path can be: 'yolov8n.pt' ho·∫∑c path t·ªõi weights custom.
    """
    global _MODEL_CACHE
    if model_path in _MODEL_CACHE:
        return _MODEL_CACHE[model_path]
    model = YOLO(model_path)
    _MODEL_CACHE[model_path] = model
    return model


# ---- OCR helper ----
def get_paddle_ocr(lang: str = "en"):
    """
    Return PaddleOCR instance. lang v√≠ d·ª•: "en", "vi" (n·∫øu c√≥ model).
    """
    if not PADDLE_AVAILABLE:
        raise RuntimeError("PaddleOCR kh√¥ng ƒë∆∞·ª£c c√†i. Ch·∫°y: pip install paddleocr")
    # use_angle_cls=False v√¨ ta t·ª± x·ª≠ l√Ω xoay 0¬∞ / 180¬∞
    return PaddleOCR(lang=lang, use_angle_cls=False)


def paddle_ocr_text_conf(ocr, image: np.ndarray):
    """
    OCR b·∫±ng Paddle, tr·∫£ v·ªÅ (text, avg_conf).
    image: BGR (OpenCV)
    """
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    result = ocr.ocr(img_rgb, cls=False)
    texts = []
    confs = []
    for line in result:
        for rec in line:
            if len(rec) >= 2:
                txt, conf = rec[1][0], rec[1][1]
                texts.append(txt.strip())
                confs.append(float(conf))
    text = " ".join(texts).strip()
    avg_conf = float(np.mean(confs)) if confs else 0.0
    return text, avg_conf


# Fallback OCR using pytesseract (n·∫øu c·∫ßn)
def ocr_with_tesseract(image: np.ndarray) -> str:
    try:
        import pytesseract
    except Exception:
        raise RuntimeError(
            "pytesseract ch∆∞a c√†i. Ch·∫°y: pip install pytesseract "
            "v√† c√†i tesseract-ocr tr√™n h·ªá th·ªëng"
        )
    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, th = cv2.threshold(
        img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
    )
    text = pytesseract.image_to_string(th, config="--psm 7")
    return text.strip()


def rotate_keep_size(img: np.ndarray, angle: float) -> np.ndarray:
    """Xoay ·∫£nh quanh t√¢m, gi·ªØ nguy√™n k√≠ch th∆∞·ªõc."""
    (h, w) = img.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(
        img, M, (w, h),
        flags=cv2.INTER_CUBIC,
        borderMode=cv2.BORDER_REPLICATE
    )
    return rotated


def ocr_plate_multi_orient(ocr, image_bgr: np.ndarray, use_paddle: bool = True):
    """
    Th·ª≠ OCR ·ªü 2 g√≥c (0¬∞, 180¬∞), ch·ªçn text c√≥ score t·ªët nh·∫•t.
    Tr·∫£ v·ªÅ (best_text, best_angle_deg).
    """
    candidates = []

    for angle in (0, 180):
        img_rot = image_bgr if angle == 0 else rotate_keep_size(image_bgr, angle)

        text = ""
        score = 0.0

        # ∆Øu ti√™n PaddleOCR n·∫øu c√≥
        if use_paddle and ocr is not None:
            try:
                text, conf = paddle_ocr_text_conf(ocr, img_rot)
                score = conf
            except Exception:
                text, score = "", 0.0

        # N·∫øu ch∆∞a c√≥ text ‚Üí th·ª≠ Tesseract
        if not text:
            try:
                t_text = ocr_with_tesseract(img_rot)
                t_score = len(t_text) / 10.0  # score t·∫°m theo ƒë·ªô d√†i
                if t_score > score:
                    text, score = t_text, t_score
            except Exception:
                pass

        # c·ªông th√™m ƒëi·ªÉm d·ª±a tr√™n s·ªë k√Ω t·ª± ch·ªØ/s·ªë (gi·ªëng bi·ªÉn s·ªë)
        plate_chars = sum(c.isalnum() for c in text)
        score += 0.05 * plate_chars

        candidates.append((score, angle, text))

    best_score, best_angle, best_text = max(candidates, key=lambda x: x[0])
    return best_text, best_angle


# ---- Chu·∫©n ho√° & t√°ch nhi·ªÅu bi·ªÉn trong 1 chu·ªói OCR ----
VN_PLATE_RE = re.compile(r'^[0-9]{2}[A-Z]-[0-9]{3}\.[0-9]{2}$')


def normalize_plate_token(token: str) -> str | None:
    """
    Nh·∫≠n 1 token OCR (vd: 'SOA-696.96') ->
    c·ªë g·∫Øng s·ª≠a th√†nh bi·ªÉn h·ª£p l·ªá (vd: '60A-696.96') ho·∫∑c tr·∫£ v·ªÅ None.
    """
    token = token.strip().upper()
    # gi·ªØ l·∫°i ch·ªâ A-Z, 0-9, '-', '.'
    token = re.sub(r'[^A-Z0-9\-.]', '', token)
    if not token:
        return None

    chars = list(token)

    # V·ªã tr√≠ ph·∫£i l√† s·ªë trong d·∫°ng NN L - NNN . NN
    digit_positions = [0, 1, 4, 5, 6, 8, 9]

    # C√°c k√Ω t·ª± hay b·ªã OCR nh·∫ßm
    char_digit_map = {
        "O": "0",
        "Q": "0",
        "D": "0",
        "I": "1",
        "L": "1",
        "Z": "2",
        "S": "6",
        "B": "8",
        "G": "6",
    }

    # S·ª≠a c√°c v·ªã tr√≠ ƒë√°ng ra ph·∫£i l√† s·ªë
    for i in digit_positions:
        if i < len(chars):
            c = chars[i]
            if (not c.isdigit()) and c in char_digit_map:
                chars[i] = char_digit_map[c]

    cand = "".join(chars)

    # Ki·ªÉm tra c√≥ ƒë√∫ng format bi·ªÉn VN kh√¥ng
    if VN_PLATE_RE.match(cand):
        return cand

    return None


def extract_plate_strings(text: str):
    """
    Nh·∫≠n 1 chu·ªói OCR (vd: 'SOA-696.96 36A-490.53') ->
    tr·∫£ v·ªÅ list c√°c bi·ªÉn ƒë√£ chu·∫©n ho√°: ['60A-696.96', '36A-490.53']
    """
    if not text:
        return []

    parts = re.split(r"\s+", text.upper())
    plates = []
    for part in parts:
        norm = normalize_plate_token(part)
        if norm and norm not in plates:
            plates.append(norm)
    return plates


# ---- PREPROCESS: C·∫ÆT 2 N·ª¨A + XOAY TR√ÅI/PH·∫¢I ----
def split_and_rotate_two_plates(image_bgr: np.ndarray, base_stem: str):
    """
    ·∫¢nh ƒë·∫ßu v√†o ch·ª©a 2 bi·ªÉn s·ªë (nh∆∞ v√≠ d·ª• b·∫°n g·ª≠i):
    - C·∫Øt ƒë√¥i theo chi·ªÅu d·ªçc.
    - N·ª≠a tr√°i xoay 90¬∞ theo chi·ªÅu kim ƒë·ªìng h·ªì (ƒë·ªÉ text n·∫±m ngang).
    - N·ª≠a ph·∫£i xoay 90¬∞ ng∆∞·ª£c chi·ªÅu kim ƒë·ªìng h·ªì.

    Tr·∫£ v·ªÅ list: [("left", img_left_rotated), ("right", img_right_rotated)]
    ƒê·ªìng th·ªùi l∆∞u debug v√†o outputs/preprocessed/.
    """
    h, w = image_bgr.shape[:2]
    mid = w // 2

    left = image_bgr[:, :mid].copy()
    right = image_bgr[:, mid:].copy()

    # V·ªõi ·∫£nh b·∫°n g·ª≠i, CW cho b√™n tr√°i, CCW cho b√™n ph·∫£i l√† ƒë·ªçc ƒë·∫πp nh·∫•t
    left_rot = cv2.rotate(left, cv2.ROTATE_90_CLOCKWISE)
    right_rot = cv2.rotate(right, cv2.ROTATE_90_COUNTERCLOCKWISE)

    # L∆∞u debug
    pre_dir = Path(OUT_DIR_DEFAULT) / "preprocessed"
    pre_dir.mkdir(parents=True, exist_ok=True)
    cv2.imwrite(str(pre_dir / f"{base_stem}_left_rot.jpg"), left_rot)
    cv2.imwrite(str(pre_dir / f"{base_stem}_right_rot.jpg"), right_rot)

    return [("left", left_rot), ("right", right_rot)]


# ---- Detection + OCR pipeline ----
def detect_and_ocr(
    model_path: str,
    image_path: str | None = None,
    image_bgr: np.ndarray | None = None,
    conf: float = 0.25,
    iou: float = 0.45,
    ocr_lang: str = "en",
    use_paddle: bool = True,
    name_suffix: str = "",
) -> List[Dict]:
    """
    Ch·∫°y YOLOv8 + OCR tr√™n 1 ·∫£nh (ƒë√£ load s·∫µn ho·∫∑c t·ª´ path).

    Tr·∫£ v·ªÅ list detections:
      [
        {
          'box': [x1,y1,x2,y2],
          'conf': float | None,
          'class_id': int | None,
          'crop_path': str,
          'raw_text': str,   # full OCR chu·ªói
          'text': str,       # 1 bi·ªÉn ƒë√£ chu·∫©n ho√° (vd: '60A-696.96')
          'angle_used': 0 ho·∫∑c 180
        },
        ...
      ]
    """
    model = get_yolo_model(model_path)

    # Load ·∫£nh n·∫øu ch∆∞a c√≥
    if image_bgr is None:
        if image_path is None:
            raise ValueError("C·∫ßn image_path ho·∫∑c image_bgr")
        image_bgr = cv2.imread(image_path)
        if image_bgr is None:
            raise FileNotFoundError(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}")

    img_bgr = image_bgr
    h, w = img_bgr.shape[:2]

    # Predict v·ªõi ultralytics YOLO (d√πng ndarray, kh√¥ng d√πng path n·ªØa)
    results = model.predict(
        source=img_bgr, conf=conf, iou=iou, verbose=False, save=False
    )
    r = results[0]

    # Chu·∫©n b·ªã OCR
    ocr = None
    if use_paddle:
        if PADDLE_AVAILABLE:
            ocr = get_paddle_ocr(lang=ocr_lang)
        else:
            print("‚ö†Ô∏è PaddleOCR kh√¥ng c√†i ‚Äî s·∫Ω fallback sang pytesseract n·∫øu c√≥.")
            ocr = None

    detections: List[Dict] = []
    out_dir = Path(OUT_DIR_DEFAULT) / "crops"
    out_dir.mkdir(parents=True, exist_ok=True)

    # L·∫•y boxes t·ª´ YOLO
    boxes = []
    if hasattr(r, "boxes") and len(r.boxes) > 0:
        for box in r.boxes:
            xy = box.xyxy[0].cpu().numpy().astype(int)
            cls_id = int(box.cls[0].cpu().numpy()) if hasattr(box, "cls") else None
            conf_val = (
                float(box.conf[0].cpu().numpy()) if hasattr(box, "conf") else None
            )
            x1, y1, x2, y2 = xy.tolist()
            # clamp v√†o trong ·∫£nh
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w - 1, x2), min(h - 1, y2)
            boxes.append((x1, y1, x2, y2, conf_val, cls_id))

    # N·∫øu model kh√¥ng detect ƒë∆∞·ª£c g√¨, d√πng heuristic ƒë∆°n gi·∫£n (optional)
    if len(boxes) == 0:
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 100, 200)
        contours, _ = cv2.findContours(
            edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE
        )
        candidates = []
        for cnt in contours:
            x, y, ww, hh = cv2.boundingRect(cnt)
            ar = ww / (hh + 1e-6)
            area = ww * hh
            if 2000 < area < w * h * 0.5 and 2.0 < ar < 6.5:
                candidates.append((x, y, x + ww, y + hh, 0.5, None))
        boxes = candidates

    # V·∫´n kh√¥ng c√≥ box ‚áí OCR lu√¥n c·∫£ ·∫£nh
    if len(boxes) == 0:
        boxes = [(0, 0, w - 1, h - 1, 1.0, None)]

    seen_texts = set()  # tr√°nh tr√πng bi·ªÉn gi·ªØa c√°c box

    base_stem = Path(image_path).stem if image_path else "image"

    # Crop t·ª´ng box v√† OCR
    for idx, (x1, y1, x2, y2, conf_val, cls_id) in enumerate(boxes):
        crop = img_bgr[y1:y2, x1:x2].copy()
        Hc, Wc = crop.shape[:2]

        # ph√≥ng to crop n·∫øu qu√° nh·ªè
        if min(Hc, Wc) < 40:
            scale = max(1, int(80 / max(Hc, Wc)))
            crop = cv2.resize(
                crop,
                (Wc * scale, Hc * scale),
                interpolation=cv2.INTER_CUBIC,
            )

        # tƒÉng t∆∞∆°ng ph·∫£n nh·∫π (CLAHE)
        lab = cv2.cvtColor(crop, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        lab = cv2.merge((l, a, b))
        crop_enh = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

        # OCR nhi·ªÅu g√≥c (0¬∞ & 180¬∞) ‚Üí chu·ªói raw_text t·ªët nh·∫•t
        raw_text, best_angle = ocr_plate_multi_orient(
            ocr, crop_enh, use_paddle=use_paddle
        )

        # T√°ch th√†nh c√°c bi·ªÉn chu·∫©n ho√°
        plate_list = extract_plate_strings(raw_text)
        if not plate_list and raw_text:
            # Kh√¥ng match pattern th√¨ gi·ªØ nguy√™n 1 chu·ªói
            plate_list = [raw_text]

        # L∆∞u crop
        crop_name = f"{base_stem}{name_suffix}_crop_{idx}.jpg"
        crop_path = out_dir / crop_name
        cv2.imwrite(str(crop_path), crop_enh)

        # M·ªói plate l√† 1 detection ri√™ng
        for plate_text in plate_list:
            if not plate_text:
                continue
            if plate_text in seen_texts:
                continue
            seen_texts.add(plate_text)

            detections.append(
                {
                    "box": [int(x1), int(y1), int(x2), int(y2)],
                    "conf": float(conf_val) if conf_val is not None else None,
                    "class_id": int(cls_id) if cls_id is not None else None,
                    "crop_path": str(crop_path),
                    "raw_text": raw_text,
                    "text": plate_text,
                    "angle_used": int(best_angle),
                }
            )

    return detections


# ---- Utility: process folder ----
def process_source(
    model_path: str,
    source: str,
    out_dir: str = OUT_DIR_DEFAULT,
    conf: float = CONF_DEFAULT,
    ocr_lang: str = OCR_LANG_DEFAULT,
    use_paddle: bool = USE_PADDLE_DEFAULT,
):
    """
    source: single image path ho·∫∑c folder.
    ·ªû ƒë√¢y c√≥ th√™m b∆∞·ªõc:
      - ƒê·ªçc ·∫£nh g·ªëc
      - C·∫Øt ƒë√¥i + xoay tr√°i/ph·∫£i
      - Ch·∫°y detect_and_ocr cho t·ª´ng n·ª≠a.
    """
    p = Path(source)
    images = []
    if p.is_dir():
        for ext in ("*.jpg", "*.jpeg", "*.png", "*.bmp", "*.png"):
            images.extend(sorted(p.glob(ext)))
    else:
        images = [p]

    results_all = {}
    Path(out_dir).mkdir(parents=True, exist_ok=True)

    for img_path in tqdm(images, desc="Processing images"):
        try:
            # ƒê·ªçc ·∫£nh g·ªëc
            img_bgr = cv2.imread(str(img_path))
            if img_bgr is None:
                raise FileNotFoundError(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_path}")

            base_stem = Path(img_path).stem

            # C·∫ÆT ƒê√îI + XOAY
            sub_images = split_and_rotate_two_plates(img_bgr, base_stem)

            results_per_image = {}

            for suffix, sub_img in sub_images:
                detections = detect_and_ocr(
                    model_path=str(model_path),
                    image_path=str(img_path),   # ch·ªâ ƒë·ªÉ ƒë·∫∑t t√™n crop
                    image_bgr=sub_img,
                    conf=conf,
                    ocr_lang=ocr_lang,
                    use_paddle=use_paddle,
                    name_suffix=f"_{suffix}",
                )
                results_per_image[suffix] = detections

                # In nhanh bi·ªÉn s·ªë ƒë·ªçc ƒë∆∞·ª£c
                if detections:
                    texts = [d["text"] for d in detections if d.get("text")]
                    if texts:
                        print(f"\nüìå {img_path} [{suffix}]: {texts}")
                    else:
                        print(f"\nüìå {img_path} [{suffix}]: (kh√¥ng ƒë·ªçc ƒë∆∞·ª£c text)")
                else:
                    print(f"\nüìå {img_path} [{suffix}]: (kh√¥ng detect ƒë∆∞·ª£c bi·ªÉn s·ªë)")

            results_all[str(img_path)] = results_per_image

        except Exception as ex:
            print(f"‚ùó L·ªói x·ª≠ l√Ω {img_path}: {ex}")
            results_all[str(img_path)] = {"error": str(ex)}

    # save json results
    json_path = Path(out_dir) / "results.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results_all, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Ho√†n t·∫•t. K·∫øt qu·∫£ l∆∞u ·ªü: {json_path}")
    return results_all


# ---- main: t·∫•t c·∫£ d√πng m·∫∑c ƒë·ªãnh ----
def main():
    print("===== Bi·ªÉn s·ªë xe: YOLOv8 + OCR (ch·∫°y m·∫∑c ƒë·ªãnh) =====")
    print(f"üîπ Model:   {MODEL_DEFAULT}")
    print(f"üîπ Source:  {SOURCE_DEFAULT}")
    print(f"üîπ Output:  {OUT_DIR_DEFAULT}")
    print(f"üîπ Conf:    {CONF_DEFAULT}")
    print(f"üîπ OCR lang:{OCR_LANG_DEFAULT}")
    print("==============================================\n")

    process_source(
        model_path=MODEL_DEFAULT,
        source=SOURCE_DEFAULT,
        out_dir=OUT_DIR_DEFAULT,
        conf=CONF_DEFAULT,
        ocr_lang=OCR_LANG_DEFAULT,
        use_paddle=USE_PADDLE_DEFAULT,
    )


if __name__ == "__main__":
    main()
